{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "* implement intersection of union (done)\n",
    "* non-max suppression\n",
    "  * takes in bounding boxes and two threshold values\n",
    "  * returns bounding boxes after filtering\n",
    "* mean average precision\n",
    "* Load data\n",
    "* Train\n",
    "* Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kanna/.local/share/virtualenvs/temp-LjgqwdTU/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/1254370/reimport-a-module-while-interactive\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import transforms\n",
    "from utils import (\n",
    "    intersection_over_union,\n",
    "    get_bboxes,\n",
    "    non_max_suppression,\n",
    "    mean_average_precision,\n",
    ")\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of Yolo Loss Function from the original yolo paper\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import intersection_over_union\n",
    "\n",
    "\n",
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the loss for yolo (v1) model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S=7, B=2, C=20):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        \"\"\"\n",
    "        S is split size of image (in paper 7),\n",
    "        B is number of boxes (in paper 2),\n",
    "        C is number of classes (in paper and VOC dataset is 20),\n",
    "        \"\"\"\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        # These are from Yolo paper, signifying how much we should\n",
    "        # pay loss for no object (noobj) and the box coordinates (coord)\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        # predictions are shaped (BATCH_SIZE, S*S(C+B*5) when inputted\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
    "\n",
    "        # Calculate IoU for the two predicted bounding boxes with target bbox\n",
    "        iou_b1 = intersection_over_union(predictions[..., 21:25], target[..., 21:25])\n",
    "        iou_b2 = intersection_over_union(predictions[..., 26:30], target[..., 21:25])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "\n",
    "        # Take the box with highest IoU out of the two prediction\n",
    "        # Note that bestbox will be indices of 0, 1 for which bbox was best\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "        exists_box = target[..., 20].unsqueeze(3)  # in paper this is Iobj_i\n",
    "\n",
    "        # ======================== #\n",
    "        #   FOR BOX COORDINATES    #\n",
    "        # ======================== #\n",
    "\n",
    "        # Set boxes with no object in them to 0. We only take out one of the two \n",
    "        # predictions, which is the one with highest Iou calculated previously.\n",
    "        box_predictions = exists_box * (\n",
    "            (\n",
    "                bestbox * predictions[..., 26:30]\n",
    "                + (1 - bestbox) * predictions[..., 21:25]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        box_targets = exists_box * target[..., 21:25]\n",
    "\n",
    "        # Take sqrt of width, height of boxes to ensure that\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
    "        )\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions, end_dim=-2),\n",
    "            torch.flatten(box_targets, end_dim=-2),\n",
    "        )\n",
    "\n",
    "        # ==================== #\n",
    "        #   FOR OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "\n",
    "        # pred_box is the confidence score for the bbox with highest IoU\n",
    "        pred_box = (\n",
    "            bestbox * predictions[..., 25:26] + (1 - bestbox) * predictions[..., 20:21]\n",
    "        )\n",
    "\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box * pred_box),\n",
    "            torch.flatten(exists_box * target[..., 20:21]),\n",
    "        )\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "\n",
    "        #max_no_obj = torch.max(predictions[..., 20:21], predictions[..., 25:26])\n",
    "        #no_object_loss = self.mse(\n",
    "        #    torch.flatten((1 - exists_box) * max_no_obj, start_dim=1),\n",
    "        #    torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
    "        #)\n",
    "\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1)\n",
    "        )\n",
    "\n",
    "        # ================== #\n",
    "        #   FOR CLASS LOSS   #\n",
    "        # ================== #\n",
    "\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :20], end_dim=-2,),\n",
    "            torch.flatten(exists_box * target[..., :20], end_dim=-2,),\n",
    "        )\n",
    "\n",
    "        loss = (\n",
    "            self.lambda_coord * box_loss  # first two rows in paper\n",
    "            + object_loss  # third row in paper\n",
    "            + self.lambda_noobj * no_object_loss  # forth row\n",
    "            + class_loss  # fifth row\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architectures = {\n",
    "    # https://arxiv.org/pdf/1506.02640.pdf\n",
    "    \"yolov1\": [\n",
    "        # (kernel_width, kernel_height filters, stride)\n",
    "        (7, 64, 2, 3),\n",
    "        # maxpooling\n",
    "        \"M\",\n",
    "        (3, 192, 1, 1),\n",
    "        \"M\",\n",
    "        (1, 128, 1, 0),\n",
    "        (3, 256, 1, 1),\n",
    "        (1, 256, 1, 0),\n",
    "        (3, 512, 1, 1),\n",
    "        \"M\",\n",
    "        # repeats\n",
    "        [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
    "        (1, 512, 1, 0),\n",
    "        (3, 1024, 1, 1),\n",
    "        \"M\",\n",
    "        [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n",
    "        (3, 1024, 1, 1),\n",
    "        (3, 1024, 2, 1),\n",
    "        (3, 1024, 1, 1),\n",
    "        (3, 1024, 1, 1),\n",
    "    ]\n",
    "}\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channals, out_channels, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_channals, out_channels=out_channels, bias=False, **kwargs)\n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.leaky_relu(self.batch_norm(self.conv(x)))\n",
    "\n",
    "class YoloV1(nn.Module):\n",
    "    def __init__(self, model_configuration, in_channels=3, **kwargs):\n",
    "        super(YoloV1, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.conv_layers = self._create_conv_layers(model_configuration)\n",
    "        self.fc_layers = self._create_fc_layers(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(torch.flatten(x, start_dim=1))\n",
    "        return x\n",
    "\n",
    "    def  _create_conv_from_layer(self, layer):\n",
    "        kernel_size, filters, stride, padding = layer\n",
    "        return CNNBlock(\n",
    "            self.in_channels,\n",
    "            filters,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding\n",
    "        ), filters\n",
    "\n",
    "    def _create_conv_layers(self, model_configuration):\n",
    "        layers = []\n",
    "        for layer in model_configuration:\n",
    "            if isinstance(layer, tuple):\n",
    "                conv_layer, out_channels = self._create_conv_from_layer(layer)\n",
    "                layers.append(conv_layer)\n",
    "                self.in_channels = out_channels\n",
    "\n",
    "            elif isinstance(layer, str):\n",
    "                if layer == \"M\":\n",
    "                    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "                elif layer == \"U\":\n",
    "                    layers.append(nn.Upsample(scale_factor=2))\n",
    "\n",
    "            elif isinstance(layer, list):\n",
    "                repeats = layer[2]\n",
    "                for _ in range(repeats):\n",
    "                    for conv_layer in layer[:2]:\n",
    "                        conv_layer, out_channels = self._create_conv_from_layer(conv_layer)\n",
    "                        layers.append(conv_layer)\n",
    "                        self.in_channels = out_channels\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _create_fc_layers(self, split_size=7, num_classes=20, num_boxes=2):\n",
    "        print(f\"{self.in_channels}\")\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.in_channels * split_size * split_size, 496),\n",
    "            nn.Dropout(0.0),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(496, split_size**2 * (5 * num_boxes + num_classes)),\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, x, boxes):\n",
    "        for t in self.transforms:\n",
    "            x, boxes = t(x), boxes\n",
    "        return x, boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from math import floor\n",
    "        \n",
    "class VOCDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        csv_file, \n",
    "        label_dir,\n",
    "        image_dir, \n",
    "        transform=None, \n",
    "        split_size=7,\n",
    "        num_boxes=2,\n",
    "        num_classes=20,\n",
    "    ):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.label_dir = label_dir\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.split_size = split_size\n",
    "        self.num_bboxes = num_boxes\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # read labels\n",
    "        label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n",
    "        boxes = []\n",
    "        for label in open(label_path).read().splitlines():\n",
    "            class_type, x, y, w, h = [\n",
    "                # if errors then fix this\n",
    "                float(value) for value in label.split()\n",
    "            ]\n",
    "\n",
    "            boxes.append([class_type, x, y, w, h])\n",
    "\n",
    "        image_path = os.path.join(self.image_dir, self.annotations.iloc[index, 0])\n",
    "        image = Image.open(image_path)\n",
    "        boxes = torch.tensor(boxes)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image, boxes = self.transform(image, boxes)\n",
    "        else:\n",
    "            image = torch.tensor(np.array(image), dtype=torch.float32)\n",
    "\n",
    "        label_tensor = torch.zeros(\n",
    "            (self.split_size, \n",
    "            self.split_size, \n",
    "            self.num_classes + 5 * self.num_bboxes))\n",
    "\n",
    "        for box in boxes:\n",
    "            class_type, x, y, w, h = box.tolist()\n",
    "            class_type = int(class_type)\n",
    "\n",
    "            width = w * self.split_size\n",
    "            height = h * self.split_size\n",
    "\n",
    "            x_index = floor(x * self.split_size)\n",
    "            y_index = floor(y * self.split_size)\n",
    "            x_relative = (x * self.split_size) - x_index\n",
    "            y_relative = (y * self.split_size) - y_index\n",
    "\n",
    "\n",
    "            # restricting every cell to only have one bbox\n",
    "            if label_tensor[y_index, x_index, 20] == 0:\n",
    "                label_tensor[y_index, x_index, 20] = 1\n",
    "                label_tensor[y_index, x_index, class_type] = 1\n",
    "\n",
    "                box_property = torch.tensor([x_relative, y_relative, width, height])\n",
    "                label_tensor[y_index, x_index, 21:25] = box_property\n",
    "\n",
    "        return image, label_tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. loading data and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, loss_fn, optimizer, device):\n",
    "    loader = tqdm.tqdm(train_loader, leave=True)\n",
    "    model.train()\n",
    "\n",
    "    mean_loss = []\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        loader.set_description(f\"Train loss: {loss.item():.4f}\")\n",
    "        #loader.postfix(loss=round(loss.item(), 4))\n",
    "\n",
    "        mean_loss.append(loss.item())\n",
    "    \n",
    "    print(f\"Mean train loss: {sum(mean_loss)/len(mean_loss):.4f}\")\n",
    "    \n",
    "\n",
    "def test_fn(test_loader, model, device):\n",
    "    model.eval()\n",
    "\n",
    "    loader = tqdm.tqdm(test_loader, leave=True)\n",
    "    loader.set_description(\"Testing\")\n",
    "    pred_boxes, target_boxes = get_bboxes(\n",
    "        loader, model, iou_threshold=0.5, threshold=0.4, device=device\n",
    "    )\n",
    "\n",
    "    mean_avg_prec = mean_average_precision(\n",
    "        pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n",
    "    )\n",
    "    print(f\"Train mAP: {mean_avg_prec}\")\n",
    "\n",
    "def save_model(model, optimizer, model_path):\n",
    "    state_dict = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(state_dict, model_path)\n",
    "    print(\"==> Saved model state to {}\".format(model_path))\n",
    "\n",
    "def load_model(model, optimizer, model_path):\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(state_dict[\"optimizer_state_dict\"])\n",
    "    print(\"==> Loaded model state from {}\".format(model_path))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "==> Loaded model state from ./trained_models/yolov1.pt\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 15/15 [00:09<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mAP: 0.11399567127227783\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 15/15 [00:06<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mAP: 0.11399567127227783\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   7%|▋         | 1/15 [00:02<00:30,  2.16s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[39mif\u001b[39;00m hyperparams[\u001b[39m\"\u001b[39m\u001b[39msave_model\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     83\u001b[0m         save_model(model, optimizer, MODEL_PATH)\n\u001b[0;32m---> 85\u001b[0m main()\n",
      "Cell \u001b[0;32mIn[7], line 80\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m         save_model(model, optimizer, MODEL_PATH)\n\u001b[1;32m     79\u001b[0m     \u001b[39m#test_fn(test_loader, model, hyperparams[\"device\"])\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     test_fn(test_loader, model, hyperparams[\u001b[39m\"\u001b[39;49m\u001b[39mdevice\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     82\u001b[0m \u001b[39mif\u001b[39;00m hyperparams[\u001b[39m\"\u001b[39m\u001b[39msave_model\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     83\u001b[0m     save_model(model, optimizer, MODEL_PATH)\n",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m, in \u001b[0;36mtest_fn\u001b[0;34m(test_loader, model, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m loader \u001b[39m=\u001b[39m tqdm\u001b[39m.\u001b[39mtqdm(test_loader, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m loader\u001b[39m.\u001b[39mset_description(\u001b[39m\"\u001b[39m\u001b[39mTesting\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m pred_boxes, target_boxes \u001b[39m=\u001b[39m get_bboxes(\n\u001b[1;32m     29\u001b[0m     loader, model, iou_threshold\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m, threshold\u001b[39m=\u001b[39;49m\u001b[39m0.4\u001b[39;49m, device\u001b[39m=\u001b[39;49mdevice\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m mean_avg_prec \u001b[39m=\u001b[39m mean_average_precision(\n\u001b[1;32m     33\u001b[0m     pred_boxes, target_boxes, iou_threshold\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, box_format\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmidpoint\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrain mAP: \u001b[39m\u001b[39m{\u001b[39;00mmean_avg_prec\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/python/notebooks/object_detection/utils.py:259\u001b[0m, in \u001b[0;36mget_bboxes\u001b[0;34m(loader, model, iou_threshold, threshold, pred_format, box_format, device)\u001b[0m\n\u001b[1;32m    256\u001b[0m     predictions \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m    258\u001b[0m batch_size \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 259\u001b[0m true_bboxes \u001b[39m=\u001b[39m cellboxes_to_boxes(labels)\n\u001b[1;32m    260\u001b[0m bboxes \u001b[39m=\u001b[39m cellboxes_to_boxes(predictions)\n\u001b[1;32m    262\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch_size):\n",
      "File \u001b[0;32m~/Documents/python/notebooks/object_detection/utils.py:328\u001b[0m, in \u001b[0;36mcellboxes_to_boxes\u001b[0;34m(out, S)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcellboxes_to_boxes\u001b[39m(out, S\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m):\n\u001b[0;32m--> 328\u001b[0m     converted_pred \u001b[39m=\u001b[39m convert_cellboxes(out)\u001b[39m.\u001b[39mreshape(out\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], S \u001b[39m*\u001b[39m S, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    329\u001b[0m     converted_pred[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m converted_pred[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mlong()\n\u001b[1;32m    330\u001b[0m     all_bboxes \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/Documents/python/notebooks/object_detection/utils.py:301\u001b[0m, in \u001b[0;36mconvert_cellboxes\u001b[0;34m(predictions, S)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_cellboxes\u001b[39m(predictions, S\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m):\n\u001b[1;32m    291\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m    Converts bounding boxes output from Yolo with\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39m    an image split size of S into entire image ratios\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39m    by one, resulting in a slower but more readable implementation.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m     predictions \u001b[39m=\u001b[39m predictions\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    302\u001b[0m     batch_size \u001b[39m=\u001b[39m predictions\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    303\u001b[0m     predictions \u001b[39m=\u001b[39m predictions\u001b[39m.\u001b[39mreshape(batch_size, \u001b[39m7\u001b[39m, \u001b[39m7\u001b[39m, \u001b[39m30\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 100,\n",
    "    \"num_workers\": 2,\n",
    "    \"weight_decay\": 0,\n",
    "    \"img_dir\": \"./data/data/images\",\n",
    "    \"label_dir\": \"./data/data/labels\",\n",
    "    \"pin_memory\": True,\n",
    "    \"load_model\": True,\n",
    "    \"save_model\": False,\n",
    "    \"save_per_epochs\": 10,\n",
    "}\n",
    "\n",
    "MODEL_PATH = \"./trained_models/yolov1.pt\"\n",
    "\n",
    "def main():\n",
    "    torch.manual_seed(123)\n",
    "    if hyperparams[\"device\"] == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    model = YoloV1(model_architectures[\"yolov1\"], in_channels=3, num_classes=20, num_boxes=2, split_size=7).to(hyperparams[\"device\"])\n",
    "\n",
    "    transform = Compose([\n",
    "        transforms.Resize((448, 448)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    train_set = VOCDataset(\n",
    "        csv_file=\"train.csv\",\n",
    "        label_dir=hyperparams[\"label_dir\"],\n",
    "        image_dir=hyperparams[\"img_dir\"],\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    test_set = VOCDataset(\n",
    "        csv_file=\"test.csv\",\n",
    "        label_dir=hyperparams[\"label_dir\"],\n",
    "        image_dir=hyperparams[\"img_dir\"],\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set,\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=hyperparams[\"num_workers\"],\n",
    "        pin_memory=hyperparams[\"pin_memory\"],\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_set,\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        drop_last=True,\n",
    "        num_workers=hyperparams[\"num_workers\"],\n",
    "        pin_memory=hyperparams[\"pin_memory\"],\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=hyperparams[\"learning_rate\"], \n",
    "        weight_decay=hyperparams[\"weight_decay\"]\n",
    "    )\n",
    "\n",
    "    loss_fn = YoloLoss()\n",
    "\n",
    "    if hyperparams[\"load_model\"]:\n",
    "        load_model(model, optimizer, MODEL_PATH)\n",
    "    \n",
    "    for epoch in range(hyperparams[\"epochs\"]):\n",
    "        print(f\"Epoch: {epoch+1}\")\n",
    "        #train_fn(train_loader, model, loss_fn, optimizer, hyperparams[\"device\"])\n",
    "\n",
    "        if epoch % hyperparams[\"save_per_epochs\"] == hyperparams[\"save_per_epochs\"] - 1 and hyperparams[\"save_model\"]:\n",
    "            save_model(model, optimizer, MODEL_PATH)\n",
    "        #test_fn(test_loader, model, hyperparams[\"device\"])\n",
    "        test_fn(test_loader, model, hyperparams[\"device\"])\n",
    "\n",
    "    if hyperparams[\"save_model\"]:\n",
    "        save_model(model, optimizer, MODEL_PATH)\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp-LjgqwdTU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
